{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f091cb01",
   "metadata": {},
   "source": [
    "# Verify the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "287b3019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux gcp-ai-notebook 4.19.0-17-cloud-amd64 #1 SMP Debian 4.19.194-2 (2021-06-21) x86_64 GNU/Linux\n",
      "PRETTY_NAME=\"Debian GNU/Linux 10 (buster)\"\n",
      "NAME=\"Debian GNU/Linux\"\n",
      "VERSION_ID=\"10\"\n",
      "VERSION=\"10 (buster)\"\n",
      "VERSION_CODENAME=buster\n",
      "ID=debian\n",
      "HOME_URL=\"https://www.debian.org/\"\n",
      "SUPPORT_URL=\"https://www.debian.org/support\"\n",
      "BUG_REPORT_URL=\"https://bugs.debian.org/\"\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "uname -a\n",
    "cat /etc/*-release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb5ed6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "google-cloud-sdk-kpt is already the newest version (348.0.0-0).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "sudo apt-get install google-cloud-sdk-kpt -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d09918f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[core]\n",
      "project = zlc-test-2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Your active configuration is: [default]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gcloud config list project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65126a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[core]\n",
      "account = 597279342139-compute@developer.gserviceaccount.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Your active configuration is: [default]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gcloud config list account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d67f707",
   "metadata": {},
   "source": [
    "# Create a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "172cfb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_NAME = 'my-k8s-cluster'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3756a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME            LOCATION    MASTER_VERSION  MASTER_IP       MACHINE_TYPE   NODE_VERSION    NUM_NODES  STATUS\n",
      "my-k8s-cluster  us-east4-a  1.20.8-gke.700  35.236.211.181  n1-standard-4  1.20.8-gke.700  3          RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [compute/zone].\n",
      "WARNING: Currently VPC-native is not the default mode during cluster creation. In the future, this will become the default mode and can be disabled using `--no-enable-ip-alias` flag. Use `--[no-]enable-ip-alias` flag to suppress this warning.\n",
      "WARNING: Starting with version 1.18, clusters will have shielded GKE nodes by default.\n",
      "WARNING: Your Pod address range (`--cluster-ipv4-cidr`) can accommodate at most 1008 node(s). \n",
      "WARNING: Starting with version 1.19, newly created clusters and node-pools will have COS_CONTAINERD as the default node image when no image type is specified.\n",
      "Creating cluster my-k8s-cluster in us-east4-a...\n",
      "............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................done.\n",
      "Created [https://container.googleapis.com/v1/projects/zlc-test-2017/zones/us-east4-a/clusters/my-k8s-cluster].\n",
      "To inspect the contents of your cluster, go to: https://console.cloud.google.com/kubernetes/workload_/gcloud/us-east4-a/my-k8s-cluster?project=zlc-test-2017\n",
      "kubeconfig entry generated for my-k8s-cluster.\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$CLUSTER_NAME\"\n",
    "\n",
    "gcloud config set compute/zone us-east4-a\n",
    "\n",
    "gcloud container clusters create $1 \\\n",
    "  --project=$(gcloud config get-value project) \\\n",
    "  --cluster-version=latest \\\n",
    "  --machine-type=n1-standard-4 \\\n",
    "  --scopes compute-rw,gke-default,storage-rw \\\n",
    "  --num-nodes=3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccb2fe0",
   "metadata": {},
   "source": [
    "## Verify the cluster has been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afb96767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME            LOCATION    MASTER_VERSION  MASTER_IP       MACHINE_TYPE   NODE_VERSION    NUM_NODES  STATUS\n",
      "my-k8s-cluster  us-east4-a  1.20.8-gke.700  35.236.211.181  n1-standard-4  1.20.8-gke.700  3          RUNNING\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gcloud container clusters list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975965cb",
   "metadata": {},
   "source": [
    "## After the cluster has started, configure access credentials so you can interact with the cluster using kubectl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e187d0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching cluster endpoint and auth data.\n",
      "kubeconfig entry generated for my-k8s-cluster.\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$CLUSTER_NAME\"\n",
    "\n",
    "gcloud container clusters get-credentials $1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e874e936",
   "metadata": {},
   "source": [
    "# Deploying `TFJob` components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998df2f4",
   "metadata": {},
   "source": [
    "## Get the manifests for `TFJob` from v1.1.0 of Kubeflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba7110f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 24\n",
      "drwxr-xr-x 4 jupyter jupyter  4096 Jul 18 03:16 lab-files\n",
      "-rw-r--r-- 1 jupyter jupyter 14159 Jul 18 04:22 tf-distributed-training-kubeflow.ipynb\n",
      "drwxr-xr-x 5 jupyter jupyter  4096 Jul 18 02:59 tf-training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: destination directory \"tf-training/tf-training\" already exists\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "SRC_REPO=https://github.com/kubeflow/manifests\n",
    "kpt pkg get $SRC_REPO/tf-training@v1.1.0 tf-training\n",
    "\n",
    "ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6670c618",
   "metadata": {},
   "source": [
    "## Create a Kubernetes namespace to host the `TFJob` operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb4d950b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace/kubeflow created\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "kubectl create namespace kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21591c40",
   "metadata": {},
   "source": [
    "## Install the `TFJob` custom resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84fd28a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customresourcedefinition.apiextensions.k8s.io/tfjobs.kubeflow.org created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "kubectl apply --kustomize tf-training/tf-job-crds/base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0596c998",
   "metadata": {},
   "source": [
    "## Install the `TFJob` operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85a9aa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount/tf-job-dashboard created\n",
      "serviceaccount/tf-job-operator created\n",
      "clusterrole.rbac.authorization.k8s.io/kubeflow-tfjobs-admin created\n",
      "clusterrole.rbac.authorization.k8s.io/kubeflow-tfjobs-edit created\n",
      "clusterrole.rbac.authorization.k8s.io/kubeflow-tfjobs-view created\n",
      "clusterrole.rbac.authorization.k8s.io/tf-job-operator created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/tf-job-operator created\n",
      "service/tf-job-operator created\n",
      "deployment.apps/tf-job-operator created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: rbac.authorization.k8s.io/v1beta1 ClusterRole is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRole\n",
      "Warning: rbac.authorization.k8s.io/v1beta1 ClusterRoleBinding is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRoleBinding\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "kubectl apply --kustomize tf-training/tf-job-operator/base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65366526",
   "metadata": {},
   "source": [
    "## Verify the installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcb0b2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME              READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS        IMAGES                                                        SELECTOR\n",
      "tf-job-operator   0/1     1            0           1s    tf-job-operator   gcr.io/kubeflow-images-public/tf_operator:vmaster-ga2ae7bff   kustomize.component=tf-job-operator\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "kubectl get deployments -n kubeflow -o wide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7a2d92",
   "metadata": {},
   "source": [
    "# Creating a Cloud Storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bd1e624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating gs://zlc-test-2017-bucket/...\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$CLUSTER_NAME\"\n",
    "\n",
    "gsutil mb gs://$(gcloud config get-value project)-bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6791dacb",
   "metadata": {},
   "source": [
    "## Verify the bucket has been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "950e045f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://zlc-test-2017-bucket/\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gsutil ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127eff84",
   "metadata": {},
   "source": [
    "# Preparing `TFJob`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cf5607",
   "metadata": {},
   "source": [
    "## Download the code package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b9f0772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 36\n",
      "drwxr-xr-x 5 jupyter jupyter  4096 Jul 18 04:22 .\n",
      "drwxr-xr-x 7 jupyter jupyter  4096 Jul 17 04:02 ..\n",
      "drwxr-xr-x 2 jupyter jupyter  4096 Jul 17 04:02 .ipynb_checkpoints\n",
      "drwxr-xr-x 4 jupyter jupyter  4096 Jul 18 03:16 lab-files\n",
      "-rw-r--r-- 1 jupyter jupyter 14159 Jul 18 04:22 tf-distributed-training-kubeflow.ipynb\n",
      "drwxr-xr-x 5 jupyter jupyter  4096 Jul 18 02:59 tf-training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: destination directory \"lab-files/distributed-training-gke\" already exists\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "SRC_REPO=https://github.com/GoogleCloudPlatform/mlops-on-gcp\n",
    "kpt pkg get $SRC_REPO/workshops/mlep-qwiklabs/distributed-training-gke lab-files\n",
    "ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2318780c",
   "metadata": {},
   "source": [
    "## Verify the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "175eefd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lab-files:\n",
      "total 32K\n",
      "drwxr-xr-x 4 jupyter jupyter 4.0K Jul 18 03:16 .\n",
      "drwxr-xr-x 5 jupyter jupyter 4.0K Jul 18 04:22 ..\n",
      "-rw-r--r-- 1 jupyter jupyter  803 Jul 18 03:16 Dockerfile\n",
      "-rw------- 1 jupyter jupyter  295 Jul 18 03:16 Kptfile\n",
      "-rw-r--r-- 1 jupyter jupyter 2.7K Jul 18 03:16 README.md\n",
      "drwxr-xr-x 3 jupyter jupyter 4.0K Jul 18 03:16 distributed-training-gke\n",
      "drwxr-xr-x 2 jupyter jupyter 4.0K Jul 18 03:16 mnist\n",
      "-rw-r--r-- 1 jupyter jupyter  568 Jul 18 04:05 tfjob.yaml\n",
      "\n",
      "lab-files/distributed-training-gke:\n",
      "total 28K\n",
      "drwxr-xr-x 3 jupyter jupyter 4.0K Jul 18 03:16 .\n",
      "drwxr-xr-x 4 jupyter jupyter 4.0K Jul 18 03:16 ..\n",
      "-rw-r--r-- 1 jupyter jupyter  803 Jul 18 03:16 Dockerfile\n",
      "-rw------- 1 jupyter jupyter  310 Jul 18 03:16 Kptfile\n",
      "-rw-r--r-- 1 jupyter jupyter 2.7K Jul 18 03:16 README.md\n",
      "drwxr-xr-x 2 jupyter jupyter 4.0K Jul 18 03:16 mnist\n",
      "-rw-r--r-- 1 jupyter jupyter 1.2K Jul 18 03:16 tfjob.yaml\n",
      "\n",
      "lab-files/distributed-training-gke/mnist:\n",
      "total 20K\n",
      "drwxr-xr-x 2 jupyter jupyter 4.0K Jul 18 03:16 .\n",
      "drwxr-xr-x 3 jupyter jupyter 4.0K Jul 18 03:16 ..\n",
      "-rw-r--r-- 1 jupyter jupyter    0 Jul 18 03:16 __init__.py\n",
      "-rw-r--r-- 1 jupyter jupyter 4.1K Jul 18 03:16 main.py\n",
      "-rw-r--r-- 1 jupyter jupyter 1.4K Jul 18 03:16 model.py\n",
      "\n",
      "lab-files/mnist:\n",
      "total 20K\n",
      "drwxr-xr-x 2 jupyter jupyter 4.0K Jul 18 03:16 .\n",
      "drwxr-xr-x 4 jupyter jupyter 4.0K Jul 18 03:16 ..\n",
      "-rw-r--r-- 1 jupyter jupyter    0 Jul 18 03:16 __init__.py\n",
      "-rw-r--r-- 1 jupyter jupyter 4.1K Jul 18 03:16 main.py\n",
      "-rw-r--r-- 1 jupyter jupyter 1.4K Jul 18 03:16 model.py\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "ls -Rlah lab-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3461f133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mapiVersion\u001b[39;49;00m: kubeflow.org/v1\n",
      "\u001b[94mkind\u001b[39;49;00m: TFJob\n",
      "\u001b[94mmetadata\u001b[39;49;00m:\n",
      "  \u001b[94mname\u001b[39;49;00m: multi-worker\n",
      "\u001b[94mspec\u001b[39;49;00m:\n",
      "  \u001b[94mcleanPodPolicy\u001b[39;49;00m: None\n",
      "  \u001b[94mtfReplicaSpecs\u001b[39;49;00m:\n",
      "    \u001b[94mWorker\u001b[39;49;00m:\n",
      "      \u001b[94mreplicas\u001b[39;49;00m: 3\n",
      "      \u001b[94mtemplate\u001b[39;49;00m:\n",
      "        \u001b[94mspec\u001b[39;49;00m:\n",
      "          \u001b[94mcontainers\u001b[39;49;00m:\n",
      "            - \u001b[94mname\u001b[39;49;00m: tensorflow\n",
      "              \u001b[94mimage\u001b[39;49;00m: gcr.io/zlc-test-2017/mnist-train\n",
      "              \u001b[94margs\u001b[39;49;00m:\n",
      "                - --epochs=5\n",
      "                - --steps_per_epoch=100\n",
      "                - --per_worker_batch=64\n",
      "                - --saved_model_path=gs://zlc-test-2017-bucket/saved_model_dir\n",
      "                - --checkpoint_path=gs://zlc-test-2017-bucket/checkpoints\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "pygmentize -l yaml lab-files/tfjob.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "022955c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# Copyright 2020 Google. All Rights Reserved.\u001b[39;49;00m\n",
      "\u001b[37m#\u001b[39;49;00m\n",
      "\u001b[37m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;49;00m\n",
      "\u001b[37m# you may not use this file except in compliance with the License.\u001b[39;49;00m\n",
      "\u001b[37m# You may obtain a copy of the License at\u001b[39;49;00m\n",
      "\u001b[37m#\u001b[39;49;00m\n",
      "\u001b[37m#     http://www.apache.org/licenses/LICENSE-2.0\u001b[39;49;00m\n",
      "\u001b[37m#\u001b[39;49;00m\n",
      "\u001b[37m# Unless required by applicable law or agreed to in writing, software\u001b[39;49;00m\n",
      "\u001b[37m# distributed under the License is distributed on an \"AS IS\" BASIS,\u001b[39;49;00m\n",
      "\u001b[37m# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\u001b[39;49;00m\n",
      "\u001b[37m# See the License for the specific language governing permissions and\u001b[39;49;00m\n",
      "\u001b[37m# limitations under the License.\u001b[39;49;00m\n",
      "\u001b[37m# ==============================================================================\u001b[39;49;00m\n",
      "\u001b[33m\"\"\"An example of multi-worker training with Keras model using Strategy API.\"\"\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow_datasets\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtfds\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmnist\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mmnist\u001b[39;49;00m\n",
      "\n",
      "BUFFER_SIZE = \u001b[34m100000\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_scale\u001b[39;49;00m(image, label):\n",
      "    \u001b[33m\"\"\"Scales an image tensor.\"\"\"\u001b[39;49;00m\n",
      "    image = tf.cast(image, tf.float32)\n",
      "    image /= \u001b[34m255\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m image, label\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_is_chief\u001b[39;49;00m(task_type, task_id):\n",
      "    \u001b[33m\"\"\"Determines if the replica is the Chief.\"\"\"\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m task_type \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m \u001b[35mor\u001b[39;49;00m task_type == \u001b[33m'\u001b[39;49;00m\u001b[33mchief\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35mor\u001b[39;49;00m (\n",
      "        task_type == \u001b[33m'\u001b[39;49;00m\u001b[33mworker\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m task_id == \u001b[34m0\u001b[39;49;00m) \n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_saved_model_dir\u001b[39;49;00m(base_path, task_type, task_id):\n",
      "    \u001b[33m\"\"\"Returns a location for the SavedModel.\"\"\"\u001b[39;49;00m\n",
      "\n",
      "    saved_model_path = base_path\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m _is_chief(task_type, task_id):\n",
      "        temp_dir = os.path.join(\u001b[33m'\u001b[39;49;00m\u001b[33m/tmp\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, task_type, \u001b[36mstr\u001b[39;49;00m(task_id))\n",
      "        tf.io.gfile.makedirs(temp_dir)\n",
      "        saved_model_path = temp_dir\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m saved_model_path\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(epochs, steps_per_epoch, per_worker_batch, checkpoint_path, saved_model_path):\n",
      "    \u001b[33m\"\"\"Trains a MNIST classification model using multi-worker mirrored strategy.\"\"\"\u001b[39;49;00m\n",
      "\n",
      "    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
      "    task_type = strategy.cluster_resolver.task_type\n",
      "    task_id = strategy.cluster_resolver.task_id\n",
      "    global_batch_size = per_worker_batch * strategy.num_replicas_in_sync\n",
      "\n",
      "    \u001b[34mwith\u001b[39;49;00m strategy.scope():\n",
      "        datasets, _ = tfds.load(name=\u001b[33m'\u001b[39;49;00m\u001b[33mmnist\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, with_info=\u001b[34mTrue\u001b[39;49;00m, as_supervised=\u001b[34mTrue\u001b[39;49;00m)\n",
      "        dataset = datasets[\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].map(_scale).cache().shuffle(BUFFER_SIZE).batch(global_batch_size).repeat()\n",
      "        options = tf.data.Options()\n",
      "        options.experimental_distribute.auto_shard_policy = \\\n",
      "            tf.data.experimental.AutoShardPolicy.DATA\n",
      "        dataset = dataset.with_options(options)\n",
      "        multi_worker_model = mnist.build_and_compile_cnn_model()\n",
      "\n",
      "    callbacks = [\n",
      "        tf.keras.callbacks.experimental.BackupAndRestore(checkpoint_path)\n",
      "    ]\n",
      "\n",
      "    multi_worker_model.fit(dataset, \n",
      "                           epochs=epochs, \n",
      "                           steps_per_epoch=steps_per_epoch,\n",
      "                           callbacks=callbacks)\n",
      "\n",
      "    \n",
      "    logging.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving the trained model to: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(saved_model_path))\n",
      "    saved_model_dir = _get_saved_model_dir(saved_model_path, task_type, task_id)\n",
      "    multi_worker_model.save(saved_model_dir)\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "\n",
      "  logging.getLogger().setLevel(logging.INFO)\n",
      "  tfds.disable_progress_bar()\n",
      "\n",
      "  parser = argparse.ArgumentParser()\n",
      "  parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                      \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                      required=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                      help=\u001b[33m'\u001b[39;49;00m\u001b[33mNumber of epochs to train.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "  parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--steps_per_epoch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                      \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                      required=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                      help=\u001b[33m'\u001b[39;49;00m\u001b[33mSteps per epoch.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "  parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--per_worker_batch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                      \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                      required=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                      help=\u001b[33m'\u001b[39;49;00m\u001b[33mPer worker batch.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "  parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--saved_model_path\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                      \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "                      required=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                      help=\u001b[33m'\u001b[39;49;00m\u001b[33mTensorflow export directory.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "  parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--checkpoint_path\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                      \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "                      required=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                      help=\u001b[33m'\u001b[39;49;00m\u001b[33mTensorflow checkpoint directory.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "  args = parser.parse_args()\n",
      "\n",
      "  train(args.epochs, args.steps_per_epoch, args.per_worker_batch, \n",
      "      args.checkpoint_path, args.saved_model_path)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "pygmentize lab-files/mnist/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7989cd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# Copyright 2020 Google. All Rights Reserved.\u001b[39;49;00m\n",
      "\u001b[37m#\u001b[39;49;00m\n",
      "\u001b[37m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;49;00m\n",
      "\u001b[37m# you may not use this file except in compliance with the License.\u001b[39;49;00m\n",
      "\u001b[37m# You may obtain a copy of the License at\u001b[39;49;00m\n",
      "\u001b[37m#\u001b[39;49;00m\n",
      "\u001b[37m#     http://www.apache.org/licenses/LICENSE-2.0\u001b[39;49;00m\n",
      "\u001b[37m#\u001b[39;49;00m\n",
      "\u001b[37m# Unless required by applicable law or agreed to in writing, software\u001b[39;49;00m\n",
      "\u001b[37m# distributed under the License is distributed on an \"AS IS\" BASIS,\u001b[39;49;00m\n",
      "\u001b[37m# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\u001b[39;49;00m\n",
      "\u001b[37m# See the License for the specific language governing permissions and\u001b[39;49;00m\n",
      "\u001b[37m# limitations under the License.\u001b[39;49;00m\n",
      "\u001b[37m# ==============================================================================\u001b[39;49;00m\n",
      "\u001b[33m\"\"\"An example of multi-worker training with Keras model using Strategy API.\"\"\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mbuild_and_compile_cnn_model\u001b[39;49;00m():\n",
      "  model = tf.keras.Sequential([\n",
      "      tf.keras.Input(shape=(\u001b[34m28\u001b[39;49;00m, \u001b[34m28\u001b[39;49;00m)),\n",
      "      tf.keras.layers.Reshape(target_shape=(\u001b[34m28\u001b[39;49;00m, \u001b[34m28\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m)),\n",
      "      tf.keras.layers.Conv2D(\u001b[34m32\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\n",
      "      tf.keras.layers.Flatten(),\n",
      "      tf.keras.layers.Dense(\u001b[34m128\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\n",
      "      tf.keras.layers.Dense(\u001b[34m10\u001b[39;49;00m)\n",
      "  ])\n",
      "  model.compile(\n",
      "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=\u001b[34mTrue\u001b[39;49;00m),\n",
      "      optimizer=tf.keras.optimizers.SGD(learning_rate=\u001b[34m0.001\u001b[39;49;00m),\n",
      "      metrics=[\u001b[33m'\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "  \u001b[34mreturn\u001b[39;49;00m model\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "pygmentize lab-files/mnist/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879f4429",
   "metadata": {},
   "source": [
    "## Packaging training code in a docker image and push to Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54445407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  33.79kB\n",
      "Step 1/4 : FROM tensorflow/tensorflow:2.4.1\n",
      " ---> 45872ba1e662\n",
      "Step 2/4 : RUN pip install tensorflow_datasets\n",
      " ---> Using cache\n",
      " ---> f86add306ef2\n",
      "Step 3/4 : ADD mnist mnist\n",
      " ---> Using cache\n",
      " ---> 489857121248\n",
      "Step 4/4 : ENTRYPOINT [\"python\", \"-m\", \"mnist.main\"]\n",
      " ---> Using cache\n",
      " ---> ba6a0e7c95f2\n",
      "Successfully built ba6a0e7c95f2\n",
      "Successfully tagged gcr.io/zlc-test-2017/mnist-train:latest\n",
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/zlc-test-2017/mnist-train]\n",
      "a642f7b967ae: Preparing\n",
      "7a3361502ecc: Preparing\n",
      "097e070097db: Preparing\n",
      "e95ae1c1e1a8: Preparing\n",
      "e43210c84711: Preparing\n",
      "74dfe3df0c94: Preparing\n",
      "8e29486d090c: Preparing\n",
      "76bfe8e7e45c: Preparing\n",
      "3779360d2582: Preparing\n",
      "9f10818f1f96: Preparing\n",
      "27502392e386: Preparing\n",
      "c95d2191d777: Preparing\n",
      "74dfe3df0c94: Waiting\n",
      "8e29486d090c: Waiting\n",
      "76bfe8e7e45c: Waiting\n",
      "3779360d2582: Waiting\n",
      "9f10818f1f96: Waiting\n",
      "27502392e386: Waiting\n",
      "c95d2191d777: Waiting\n",
      "e95ae1c1e1a8: Pushed\n",
      "a642f7b967ae: Pushed\n",
      "097e070097db: Pushed\n",
      "7a3361502ecc: Pushed\n",
      "74dfe3df0c94: Pushed\n",
      "9f10818f1f96: Layer already exists\n",
      "27502392e386: Layer already exists\n",
      "8e29486d090c: Pushed\n",
      "c95d2191d777: Layer already exists\n",
      "3779360d2582: Pushed\n",
      "76bfe8e7e45c: Pushed\n",
      "e43210c84711: Pushed\n",
      "latest: digest: sha256:32bcf14192fdc5266dfaefabd3846404fd95f696fd3756904b0245195f0ba293 size: 2835\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "IMAGE_NAME=mnist-train\n",
    "\n",
    "cd lab-files\n",
    "docker build -t gcr.io/$(gcloud config get-value project)/${IMAGE_NAME} .\n",
    "docker push gcr.io/$(gcloud config get-value project)/${IMAGE_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4790171e",
   "metadata": {},
   "source": [
    "## Verify the docker image has been pushed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e4a195a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME\n",
      "gcr.io/zlc-test-2017/mnist-train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Only listing images in gcr.io/zlc-test-2017. Use --repository to list images in other repositories.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gcloud container images list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45704297",
   "metadata": {},
   "source": [
    "## Update `image` and `args` in `tfjob.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "546a8df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lab-files/tfjob.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile lab-files/tfjob.yaml\n",
    "\n",
    "apiVersion: kubeflow.org/v1\n",
    "kind: TFJob\n",
    "metadata:\n",
    "  name: multi-worker\n",
    "spec:\n",
    "  cleanPodPolicy: None\n",
    "  tfReplicaSpecs:\n",
    "    Worker:\n",
    "      replicas: 3\n",
    "      template:\n",
    "        spec:\n",
    "          containers:\n",
    "            - name: tensorflow\n",
    "              image: gcr.io/zlc-test-2017/mnist-train\n",
    "              args:\n",
    "                - --epochs=5\n",
    "                - --steps_per_epoch=100\n",
    "                - --per_worker_batch=64\n",
    "                - --saved_model_path=gs://zlc-test-2017-bucket/saved_model_dir\n",
    "                - --checkpoint_path=gs://zlc-test-2017-bucket/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8b21478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mapiVersion\u001b[39;49;00m: kubeflow.org/v1\n",
      "\u001b[94mkind\u001b[39;49;00m: TFJob\n",
      "\u001b[94mmetadata\u001b[39;49;00m:\n",
      "  \u001b[94mname\u001b[39;49;00m: multi-worker\n",
      "\u001b[94mspec\u001b[39;49;00m:\n",
      "  \u001b[94mcleanPodPolicy\u001b[39;49;00m: None\n",
      "  \u001b[94mtfReplicaSpecs\u001b[39;49;00m:\n",
      "    \u001b[94mWorker\u001b[39;49;00m:\n",
      "      \u001b[94mreplicas\u001b[39;49;00m: 3\n",
      "      \u001b[94mtemplate\u001b[39;49;00m:\n",
      "        \u001b[94mspec\u001b[39;49;00m:\n",
      "          \u001b[94mcontainers\u001b[39;49;00m:\n",
      "            - \u001b[94mname\u001b[39;49;00m: tensorflow\n",
      "              \u001b[94mimage\u001b[39;49;00m: gcr.io/zlc-test-2017/mnist-train\n",
      "              \u001b[94margs\u001b[39;49;00m:\n",
      "                - --epochs=5\n",
      "                - --steps_per_epoch=100\n",
      "                - --per_worker_batch=64\n",
      "                - --saved_model_path=gs://zlc-test-2017-bucket/saved_model_dir\n",
      "                - --checkpoint_path=gs://zlc-test-2017-bucket/checkpoints\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "pygmentize -l yaml lab-files/tfjob.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa231944",
   "metadata": {},
   "source": [
    "# Submit `TFJob`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6e67972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfjob.kubeflow.org/multi-worker created\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd lab-files\n",
    "kubectl apply -f tfjob.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42b7ecf",
   "metadata": {},
   "source": [
    "# Monitor `TFJob`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee6951d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                    READY   STATUS    RESTARTS   AGE\n",
      "multi-worker-worker-0   1/1     Running   0          60s\n",
      "multi-worker-worker-1   1/1     Running   0          60s\n",
      "multi-worker-worker-2   1/1     Running   0          60s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "sleep 1m  # wait to let pods start running\n",
    "kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "939e980c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:         multi-worker\n",
      "Namespace:    default\n",
      "Labels:       <none>\n",
      "Annotations:  <none>\n",
      "API Version:  kubeflow.org/v1\n",
      "Kind:         TFJob\n",
      "Metadata:\n",
      "  Creation Timestamp:  2021-07-18T04:24:29Z\n",
      "  Generation:          1\n",
      "  Managed Fields:\n",
      "    API Version:  kubeflow.org/v1\n",
      "    Fields Type:  FieldsV1\n",
      "    fieldsV1:\n",
      "      f:metadata:\n",
      "        f:annotations:\n",
      "          .:\n",
      "          f:kubectl.kubernetes.io/last-applied-configuration:\n",
      "      f:spec:\n",
      "        .:\n",
      "        f:cleanPodPolicy:\n",
      "        f:tfReplicaSpecs:\n",
      "          .:\n",
      "          f:Worker:\n",
      "            .:\n",
      "            f:replicas:\n",
      "            f:template:\n",
      "              .:\n",
      "              f:spec:\n",
      "    Manager:      kubectl-client-side-apply\n",
      "    Operation:    Update\n",
      "    Time:         2021-07-18T04:24:29Z\n",
      "    API Version:  kubeflow.org/v1\n",
      "    Fields Type:  FieldsV1\n",
      "    fieldsV1:\n",
      "      f:spec:\n",
      "        f:successPolicy:\n",
      "        f:tfReplicaSpecs:\n",
      "          f:Worker:\n",
      "            f:restartPolicy:\n",
      "            f:template:\n",
      "              f:metadata:\n",
      "                .:\n",
      "                f:creationTimestamp:\n",
      "              f:spec:\n",
      "                f:containers:\n",
      "      f:status:\n",
      "        .:\n",
      "        f:conditions:\n",
      "        f:replicaStatuses:\n",
      "          .:\n",
      "          f:Worker:\n",
      "            .:\n",
      "            f:active:\n",
      "        f:startTime:\n",
      "    Manager:         tf-operator.v1\n",
      "    Operation:       Update\n",
      "    Time:            2021-07-18T04:25:05Z\n",
      "  Resource Version:  2015\n",
      "  UID:               9b564563-ad01-4483-87e5-b3160a25a006\n",
      "Spec:\n",
      "  Clean Pod Policy:  None\n",
      "  Tf Replica Specs:\n",
      "    Worker:\n",
      "      Replicas:  3\n",
      "      Template:\n",
      "        Spec:\n",
      "          Containers:\n",
      "            Args:\n",
      "              --epochs=5\n",
      "              --steps_per_epoch=100\n",
      "              --per_worker_batch=64\n",
      "              --saved_model_path=gs://zlc-test-2017-bucket/saved_model_dir\n",
      "              --checkpoint_path=gs://zlc-test-2017-bucket/checkpoints\n",
      "            Image:  gcr.io/zlc-test-2017/mnist-train\n",
      "            Name:   tensorflow\n",
      "Status:\n",
      "  Conditions:\n",
      "    Last Transition Time:  2021-07-18T04:24:29Z\n",
      "    Last Update Time:      2021-07-18T04:24:29Z\n",
      "    Message:               TFJob multi-worker is created.\n",
      "    Reason:                TFJobCreated\n",
      "    Status:                True\n",
      "    Type:                  Created\n",
      "    Last Transition Time:  2021-07-18T04:25:05Z\n",
      "    Last Update Time:      2021-07-18T04:25:05Z\n",
      "    Message:               TFJob multi-worker is running.\n",
      "    Reason:                TFJobRunning\n",
      "    Status:                True\n",
      "    Type:                  Running\n",
      "  Replica Statuses:\n",
      "    Worker:\n",
      "      Active:  3\n",
      "  Start Time:  2021-07-18T04:24:29Z\n",
      "Events:\n",
      "  Type    Reason                   Age   From         Message\n",
      "  ----    ------                   ----  ----         -------\n",
      "  Normal  SuccessfulCreatePod      60s   tf-operator  Created pod: multi-worker-worker-0\n",
      "  Normal  SuccessfulCreatePod      60s   tf-operator  Created pod: multi-worker-worker-1\n",
      "  Normal  SuccessfulCreatePod      60s   tf-operator  Created pod: multi-worker-worker-2\n",
      "  Normal  SuccessfulCreateService  60s   tf-operator  Created service: multi-worker-worker-0\n",
      "  Normal  SuccessfulCreateService  60s   tf-operator  Created service: multi-worker-worker-1\n",
      "  Normal  SuccessfulCreateService  60s   tf-operator  Created service: multi-worker-worker-2\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "kubectl describe tfjob multi-worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56e0aec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                    READY   STATUS      RESTARTS   AGE\n",
      "multi-worker-worker-0   0/1     Completed   0          6m1s\n",
      "multi-worker-worker-1   0/1     Completed   0          6m1s\n",
      "multi-worker-worker-2   0/1     Completed   0          6m1s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "sleep 5m  # wait to let pods complete\n",
    "kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fdc552a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-18 04:25:05.017695: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-07-18 04:25:05.017741: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "WARNING:tensorflow:From /mnist/main.py:58: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "use distribute.MultiWorkerMirroredStrategy instead\n",
      "2021-07-18 04:25:07.566738: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-07-18 04:25:07.567063: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-07-18 04:25:07.567086: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-07-18 04:25:07.567107: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (multi-worker-worker-0): /proc/driver/nvidia/version does not exist\n",
      "2021-07-18 04:25:07.567788: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-18 04:25:07.568153: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-07-18 04:25:07.568756: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-07-18 04:25:07.573978: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> multi-worker-worker-0.default.svc:2222, 1 -> multi-worker-worker-1.default.svc:2222, 2 -> multi-worker-worker-2.default.svc:2222}\n",
      "2021-07-18 04:25:07.574404: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://multi-worker-worker-0.default.svc:2222\n",
      "INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0']\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:worker/task:0',)\n",
      "INFO:tensorflow:Waiting for the cluster, timeout = inf\n",
      "INFO:tensorflow:Cluster is ready.\n",
      "INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['multi-worker-worker-0.default.svc:2222', 'multi-worker-worker-1.default.svc:2222', 'multi-worker-worker-2.default.svc:2222']}, task_type = 'worker', task_id = 0, num_workers = 3, local_devices = ('/job:worker/task:0',), communication = CommunicationImplementation.AUTO\n",
      "INFO:absl:Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: mnist/3.0.1\n",
      "INFO:absl:Load dataset info from /tmp/tmpnk04zv33tfds\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Generating dataset mnist (/root/tensorflow_datasets/mnist/3.0.1)\n",
      "INFO:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
      "local data directory. If you'd instead prefer to read directly from our public\n",
      "GCS bucket (recommended if you're running on GCP), you can instead pass\n",
      "`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n",
      "\n",
      "INFO:absl:Load dataset info from /root/tensorflow_datasets/mnist/3.0.1.incomplete73NM6D\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split None, from /root/tensorflow_datasets/mnist/3.0.1\n",
      "2021-07-18 04:25:10.473002: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-07-18 04:25:10.473567: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2199995000 Hz\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 6 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 6 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 6 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 6 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "\u001b[1mDownloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n",
      "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 24s 125ms/step - loss: 2.2865 - accuracy: 0.1034\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 12s 122ms/step - loss: 2.1957 - accuracy: 0.3744\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 12s 121ms/step - loss: 2.0756 - accuracy: 0.6080\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 12s 121ms/step - loss: 1.9116 - accuracy: 0.6942\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 12s 123ms/step - loss: 1.7025 - accuracy: 0.7231\n",
      "INFO:root:Saving the trained model to: gs://zlc-test-2017-bucket/saved_model_dir\n",
      "2021-07-18 04:26:57.964276: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "INFO:tensorflow:Assets written to: gs://zlc-test-2017-bucket/saved_model_dir/assets\n",
      "INFO:tensorflow:Assets written to: gs://zlc-test-2017-bucket/saved_model_dir/assets\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "kubectl logs multi-worker-worker-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60f4d010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-18 04:25:04.960336: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-07-18 04:25:04.960384: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "WARNING:tensorflow:From /mnist/main.py:58: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "use distribute.MultiWorkerMirroredStrategy instead\n",
      "2021-07-18 04:25:07.729757: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-07-18 04:25:07.729976: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-07-18 04:25:07.730000: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-07-18 04:25:07.730026: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (multi-worker-worker-1): /proc/driver/nvidia/version does not exist\n",
      "2021-07-18 04:25:07.730602: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-18 04:25:07.730968: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-07-18 04:25:07.731559: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-07-18 04:25:07.737261: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> multi-worker-worker-0.default.svc:2222, 1 -> multi-worker-worker-1.default.svc:2222, 2 -> multi-worker-worker-2.default.svc:2222}\n",
      "2021-07-18 04:25:07.737658: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://multi-worker-worker-1.default.svc:2222\n",
      "INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:1/device:CPU:0']\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:worker/task:1',)\n",
      "INFO:tensorflow:Waiting for the cluster, timeout = inf\n",
      "INFO:tensorflow:Cluster is ready.\n",
      "INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['multi-worker-worker-0.default.svc:2222', 'multi-worker-worker-1.default.svc:2222', 'multi-worker-worker-2.default.svc:2222']}, task_type = 'worker', task_id = 1, num_workers = 3, local_devices = ('/job:worker/task:1',), communication = CommunicationImplementation.AUTO\n",
      "INFO:absl:Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: mnist/3.0.1\n",
      "INFO:absl:Load dataset info from /tmp/tmp4855386dtfds\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Generating dataset mnist (/root/tensorflow_datasets/mnist/3.0.1)\n",
      "INFO:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
      "local data directory. If you'd instead prefer to read directly from our public\n",
      "GCS bucket (recommended if you're running on GCP), you can instead pass\n",
      "`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n",
      "\n",
      "INFO:absl:Load dataset info from /root/tensorflow_datasets/mnist/3.0.1.incompleteSWTR2X\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split None, from /root/tensorflow_datasets/mnist/3.0.1\n",
      "2021-07-18 04:25:11.407465: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-07-18 04:25:11.407956: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2199995000 Hz\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 6 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 6 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 6 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 6 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "\u001b[1mDownloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n",
      "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 23s 125ms/step - loss: 2.2865 - accuracy: 0.1034\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 12s 122ms/step - loss: 2.1957 - accuracy: 0.3744\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 12s 121ms/step - loss: 2.0756 - accuracy: 0.6080\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 12s 121ms/step - loss: 1.9116 - accuracy: 0.6942\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 12s 123ms/step - loss: 1.7025 - accuracy: 0.7231\n",
      "INFO:root:Saving the trained model to: gs://zlc-test-2017-bucket/saved_model_dir\n",
      "2021-07-18 04:26:57.730557: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "INFO:tensorflow:Assets written to: /tmp/worker/1/assets\n",
      "INFO:tensorflow:Assets written to: /tmp/worker/1/assets\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "kubectl logs multi-worker-worker-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a531fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-18 04:25:05.004436: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-07-18 04:25:05.004482: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "WARNING:tensorflow:From /mnist/main.py:58: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "use distribute.MultiWorkerMirroredStrategy instead\n",
      "2021-07-18 04:25:07.445321: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-07-18 04:25:07.445604: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-07-18 04:25:07.445630: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-07-18 04:25:07.445670: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (multi-worker-worker-2): /proc/driver/nvidia/version does not exist\n",
      "2021-07-18 04:25:07.446566: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-18 04:25:07.446859: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-07-18 04:25:07.447400: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-07-18 04:25:07.453671: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> multi-worker-worker-0.default.svc:2222, 1 -> multi-worker-worker-1.default.svc:2222, 2 -> multi-worker-worker-2.default.svc:2222}\n",
      "2021-07-18 04:25:07.454248: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://multi-worker-worker-2.default.svc:2222\n",
      "INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:2/device:CPU:0']\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:worker/task:2',)\n",
      "INFO:tensorflow:Waiting for the cluster, timeout = inf\n",
      "INFO:tensorflow:Cluster is ready.\n",
      "INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['multi-worker-worker-0.default.svc:2222', 'multi-worker-worker-1.default.svc:2222', 'multi-worker-worker-2.default.svc:2222']}, task_type = 'worker', task_id = 2, num_workers = 3, local_devices = ('/job:worker/task:2',), communication = CommunicationImplementation.AUTO\n",
      "INFO:absl:Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: mnist/3.0.1\n",
      "INFO:absl:Load dataset info from /tmp/tmpy_b39jahtfds\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Generating dataset mnist (/root/tensorflow_datasets/mnist/3.0.1)\n",
      "INFO:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
      "local data directory. If you'd instead prefer to read directly from our public\n",
      "GCS bucket (recommended if you're running on GCP), you can instead pass\n",
      "`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n",
      "\n",
      "INFO:absl:Load dataset info from /root/tensorflow_datasets/mnist/3.0.1.incomplete41X3PQ\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split None, from /root/tensorflow_datasets/mnist/3.0.1\n",
      "2021-07-18 04:25:11.403077: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-07-18 04:25:11.403644: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2199995000 Hz\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 6 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 6 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 6 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 6 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 3, implementation = AUTO, num_packs = 1\n",
      "\u001b[1mDownloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n",
      "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 23s 125ms/step - loss: 2.2865 - accuracy: 0.1034\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 12s 122ms/step - loss: 2.1957 - accuracy: 0.3744\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 12s 121ms/step - loss: 2.0756 - accuracy: 0.6080\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 12s 121ms/step - loss: 1.9116 - accuracy: 0.6942\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 12s 123ms/step - loss: 1.7025 - accuracy: 0.7231\n",
      "INFO:root:Saving the trained model to: gs://zlc-test-2017-bucket/saved_model_dir\n",
      "2021-07-18 04:26:57.834073: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "INFO:tensorflow:Assets written to: /tmp/worker/2/assets\n",
      "INFO:tensorflow:Assets written to: /tmp/worker/2/assets\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "kubectl logs multi-worker-worker-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994addc8",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a3b5ec",
   "metadata": {},
   "source": [
    "## Delete the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39bf2a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting cluster my-k8s-cluster...\n",
      "..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................done.\n",
      "Deleted [https://container.googleapis.com/v1/projects/zlc-test-2017/zones/us-east4-a/clusters/my-k8s-cluster].\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$CLUSTER_NAME\"\n",
    "\n",
    "gcloud container clusters delete $1 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacf3d38",
   "metadata": {},
   "source": [
    "## Delete the Cloud Storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2002fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://zlc-test-2017-bucket/checkpoints/#1626582310915887...\n",
      "Removing gs://zlc-test-2017-bucket/saved_model_dir/#1626582419007994...\n",
      "Removing gs://zlc-test-2017-bucket/saved_model_dir/assets/#1626582423747345...\n",
      "Removing gs://zlc-test-2017-bucket/saved_model_dir/variables/variables.data-00000-of-00001#1626582422119167...\n",
      "Removing gs://zlc-test-2017-bucket/saved_model_dir/saved_model.pb#1626582424392744...\n",
      "Removing gs://zlc-test-2017-bucket/saved_model_dir/variables/variables.index#1626582422661051...\n",
      "Removing gs://zlc-test-2017-bucket/saved_model_dir/variables/#1626582419257911...\n",
      "/ [7/7 objects] 100% Done                                                       \n",
      "Operation completed over 7 objects.                                              \n",
      "Removing gs://zlc-test-2017-bucket/...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gsutil -m rm -r gs://zlc-test-2017-bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b181a2b",
   "metadata": {},
   "source": [
    "## Delete the pushed docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "734c2a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Implicit \":latest\" tag specified: gcr.io/zlc-test-2017/mnist-train\n",
      "Digests:\n",
      "- gcr.io/zlc-test-2017/mnist-train@sha256:32bcf14192fdc5266dfaefabd3846404fd95f696fd3756904b0245195f0ba293\n",
      "  Associated tags:\n",
      " - latest\n",
      "Tags:\n",
      "- gcr.io/zlc-test-2017/mnist-train:latest\n",
      "Deleted [gcr.io/zlc-test-2017/mnist-train:latest].\n",
      "Deleted [gcr.io/zlc-test-2017/mnist-train@sha256:32bcf14192fdc5266dfaefabd3846404fd95f696fd3756904b0245195f0ba293].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gcloud container images delete gcr.io/zlc-test-2017/mnist-train --force-delete-tags --quiet"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m75"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
